import gc
import argparse

import numpy as np
from PIL import Image

import gradio as gr
import torch
from diffusers import DiffusionPipeline

from fastdm.model_entry import create_model
from fastdm.cache_config import CacheConfig

def parseArgs():
    parser = argparse.ArgumentParser(description="Options for FastDM Server", conflict_handler='resolve')
    parser.add_argument('--steps', type=int, default=25, help="Inference steps")
    parser.add_argument('--device', type=int, default=0, help="device number")

    parser.add_argument('--use-fp8', action='store_true', help="Enable fp8 model inference")
    parser.add_argument('--use-int8', action='store_true', help="Enable int8 model inference")
    parser.add_argument('--kernel-backend', default="cuda", help="kernel backend: torch/triton/cuda")

    parser.add_argument('--model-path', default='', help="Directory for diffusion model path")

    parser.add_argument('--data-type', default="bfloat16", help="data type")
    parser.add_argument('--architecture', default="flux", help="model architecture: sdxl/flux/sd3/qwen")

    parser.add_argument('--qwen-oom-resolve', action='store_true', help="It can resolve OOM error of qwen-image model if set to true")

    # caching args
    parser.add_argument('--cache-config', type=str, default=None, help="cache config json file path")

    parser.add_argument('--port', type=int, default=7890, help="server ssh port number")

    return parser.parse_args()

class FastDMEngine:
    def __init__(self, 
                 model_path, 
                 data_type=torch.bfloat16, 
                 device_num=0, 
                 quant_type=torch.float8_e4m3fn, 
                 kernel_backend="cuda", 
                 architecture="flux", 
                 cache_config=None,
                 qwen_oom_resolve=False):

        torch.cuda.set_device(device_num)

        self.pipe = DiffusionPipeline.from_pretrained(model_path, torch_dtype=data_type, use_safetensors=True)

        if cache_config is not None:
            cache_config = CacheConfig.from_json(cache_config)
            cache_config.current_steps_callback = lambda: self.pipe.scheduler.step_index
        else:
            cache_config = None

        if "sdxl" == architecture:
           self.pipe.unet = create_model("sdxl",
                                         ckpt_path = self.pipe.unet.state_dict(),
                                         dtype=data_type, 
                                         quant_type=quant_type, 
                                         kernel_backend=kernel_backend).eval()
        elif "flux" == architecture:
            self.pipe.transformer = create_model("flux",
                                         ckpt_path = self.pipe.transformer.state_dict(),
                                         dtype=data_type, 
                                         quant_type=quant_type, 
                                         kernel_backend=kernel_backend,
                                         cache_config=cache_config).eval()
        elif "sd3" == architecture:
            self.pipe.transformer = create_model("sd3",
                                         ckpt_path = self.pipe.transformer.state_dict(),
                                         dtype=data_type, 
                                         quant_type=quant_type, 
                                         kernel_backend=kernel_backend,
                                         cache_config=cache_config).eval()
        elif "qwen" == architecture:
            self.pipe.transformer = create_model("qwen",
                                         ckpt_path = self.pipe.transformer.state_dict(),
                                         dtype=data_type, 
                                         quant_type=quant_type, 
                                         kernel_backend=kernel_backend,
                                         cache_config=cache_config,
                                         need_resolve_oom=qwen_oom_resolve).eval()
            if qwen_oom_resolve:
                import os
                import sys
                from diffusers.models.autoencoders.autoencoder_kl_qwenimage import AutoencoderKLQwenImage
                sys.path.append(os.path.join(os.path.dirname(__file__), ".."))
                from utils.qwen_vae import qwen_vae_new_decode, qwen_vae_new_encode
                AutoencoderKLQwenImage._decode = qwen_vae_new_decode
                AutoencoderKLQwenImage._encode = qwen_vae_new_encode
        else:
            raise ValueError(
                f"The {architecture} model is not supported!!!"
            )

        if "qwen" == architecture and qwen_oom_resolve:
            self.pipe.vae.to("cuda")
        else:
            self.pipe.to(f"cuda")

    def generate(self, prompt_text, src_image=None, gen_seed=42, inf_step=25, gen_width=2048, gen_height=1024, max_len=512, guidance=3.5):

        gen = torch.Generator().manual_seed(gen_seed)

        if src_image is None:
            images = self.pipe(prompt=prompt_text, num_inference_steps=inf_step, generator=gen, width=gen_width, height=gen_height, guidance_scale=guidance, max_sequence_length=max_len).images[0]
        else:
            images = self.pipe(prompt=prompt_text, image=src_image, num_inference_steps=inf_step, generator=gen, width=gen_width, height=gen_height, guidance_scale=guidance, max_sequence_length=max_len).images[0]
        
        torch.cuda.empty_cache()
        gc.collect()
        
        return images

#å¤šå›¾ç‰‡å¤„ç†å‡½æ•°
def process_multiple_images(images, blend_mode="concatenate", concat_direction="horizontal"):
    """
    å¤„ç†å¤šå¼ è¾“å…¥å›¾ç‰‡
    blend_mode: "average" - å¹³å‡æ··åˆ, "concatenate" - æ‹¼æ¥, "first" - ä½¿ç”¨ç¬¬ä¸€å¼ 
    concat_direction: "horizontal" - æ°´å¹³æ‹¼æ¥, "vertical" - å‚ç›´æ‹¼æ¥
    """
    if not images or len(images) == 0:
        return None
    
    # å¦‚æœåªæœ‰ä¸€å¼ å›¾ç‰‡ï¼Œç›´æ¥è¿”å›
    if len(images) == 1:
        return images[0]
    
    # è½¬æ¢æ‰€æœ‰å›¾ç‰‡ä¸ºPIL Imageæ ¼å¼
    pil_images = []
    for img in images:
        if isinstance(img, np.ndarray):
            pil_img = Image.fromarray(img)
        elif isinstance(img, Image.Image):
            pil_img = img
        else:
            continue
        pil_images.append(pil_img.convert("RGB"))
    
    if not pil_images:
        return None
    
    if blend_mode == "first":
        return pil_images[0]
    
    elif blend_mode == "average":
        # ç»Ÿä¸€æ‰€æœ‰å›¾ç‰‡å°ºå¯¸åˆ°ç¬¬ä¸€å¼ å›¾ç‰‡çš„å¤§å°
        base_size = pil_images[0].size
        resized_images = [img.resize(base_size, Image.Resampling.LANCZOS) for img in pil_images]
        
        # å¹³å‡æ··åˆ
        arrays = [np.array(img, dtype=np.float32) for img in resized_images]
        blended_array = np.mean(arrays, axis=0).astype(np.uint8)
        return Image.fromarray(blended_array)
    
    elif blend_mode == "concatenate":
        return concatenate_images(pil_images, concat_direction)
    
    return pil_images[0]

def concatenate_images(images, direction="horizontal"):
    """
    æ‹¼æ¥å¤šå¼ å›¾ç‰‡
    direction: "horizontal" - æ°´å¹³æ‹¼æ¥, "vertical" - å‚ç›´æ‹¼æ¥
    """
    if not images:
        return None
    
    if len(images) == 1:
        return images[0]
    
    if direction == "horizontal":
        # æ°´å¹³æ‹¼æ¥ï¼šç»Ÿä¸€é«˜åº¦ï¼Œå®½åº¦ç›¸åŠ 
        # æ‰¾åˆ°æœ€å°é«˜åº¦
        min_height = min(img.height for img in images)
        
        # è°ƒæ•´æ‰€æœ‰å›¾ç‰‡åˆ°ç›¸åŒé«˜åº¦ï¼Œä¿æŒå®½é«˜æ¯”
        resized_images = []
        total_width = 0
        
        for img in images:
            # è®¡ç®—æ–°å®½åº¦ä»¥ä¿æŒå®½é«˜æ¯”
            aspect_ratio = img.width / img.height
            new_width = int(min_height * aspect_ratio)
            resized_img = img.resize((new_width, min_height), Image.Resampling.LANCZOS)
            resized_images.append(resized_img)
            total_width += new_width
        
        # åˆ›å»ºæ–°ç”»å¸ƒ
        concatenated = Image.new('RGB', (total_width, min_height))
        
        # ç²˜è´´å›¾ç‰‡
        x_offset = 0
        for img in resized_images:
            concatenated.paste(img, (x_offset, 0))
            x_offset += img.width
            
        return concatenated
    
    elif direction == "vertical":
        # å‚ç›´æ‹¼æ¥ï¼šç»Ÿä¸€å®½åº¦ï¼Œé«˜åº¦ç›¸åŠ 
        # æ‰¾åˆ°æœ€å°å®½åº¦
        min_width = min(img.width for img in images)
        
        # è°ƒæ•´æ‰€æœ‰å›¾ç‰‡åˆ°ç›¸åŒå®½åº¦ï¼Œä¿æŒå®½é«˜æ¯”
        resized_images = []
        total_height = 0
        
        for img in images:
            # è®¡ç®—æ–°é«˜åº¦ä»¥ä¿æŒå®½é«˜æ¯”
            aspect_ratio = img.height / img.width
            new_height = int(min_width * aspect_ratio)
            resized_img = img.resize((min_width, new_height), Image.Resampling.LANCZOS)
            resized_images.append(resized_img)
            total_height += new_height
        
        # åˆ›å»ºæ–°ç”»å¸ƒ
        concatenated = Image.new('RGB', (min_width, total_height))
        
        # ç²˜è´´å›¾ç‰‡
        y_offset = 0
        for img in resized_images:
            concatenated.paste(img, (0, y_offset))
            y_offset += img.height
            
        return concatenated
    
    return images[0]

args = parseArgs()
torch.cuda.set_device(args.device)
engine_ = FastDMEngine(model_path=args.model_path, 
                       data_type=torch.bfloat16 if "bfloat16"==args.data_type else torch.float16, 
                       device_num=args.device, 
                       quant_type=torch.float8_e4m3fn if args.use_fp8 else (torch.int8 if args.use_int8 else None),
                       kernel_backend=args.kernel_backend, 
                       architecture=args.architecture, 
                       cache_config=args.cache_config,
                       qwen_oom_resolve=args.qwen_oom_resolve)

# å›¾ç‰‡ç”Ÿæˆå‡½æ•°
def generate_image_from_prompt(prompt,
                                init_images=None,
                                blend_mode="concatenate",
                                concat_direction="horizontal",  # æ–°å¢æ‹¼æ¥æ–¹å‘å‚æ•°
                                steps=50, 
                                cfg_scale=3.5, 
                                seed=-1,
                                sampler="Euler a",
                                width=512,
                                height=512,
                                pipeline=None):
    """
    è°ƒç”¨æ¨ç†å¼•æ“ç”Ÿæˆå›¾ç‰‡ï¼Œæ”¯æŒå¤šå›¾ç‰‡è¾“å…¥
    è¿”å›PILå›¾åƒå¯¹è±¡æˆ–æ–‡ä»¶è·¯å¾„
    """

    # Handle init_images - it could be a single image or a list of images
    if init_images is None:
        input_image = None
    else:
        # If it's a single image (not a list), convert to list
        if not isinstance(init_images, list):
            init_images = [init_images]
        
        images = [img[0] for img in init_images]

        input_image = process_multiple_images(images, blend_mode, concat_direction)
        if input_image:
            input_image = input_image.convert("RGB")  # ç¡®ä¿RGBæ ¼å¼

    try:
        image = engine_.generate(prompt, src_image=input_image, gen_seed=seed if seed>=0 else torch.randint(0, 10000, (1,)).item(), inf_step=steps, gen_width=width, gen_height=height, guidance=cfg_scale)
        return image
    except Exception as e:
        print(f"ç”Ÿæˆå¤±è´¥: {e}")
        return None

# åˆ›å»ºå¸¦å‚æ•°æ§åˆ¶çš„Gradioç•Œé¢
with gr.Blocks(title="FastDMç”Ÿå›¾æœåŠ¡", css=".gradio-container {max-width: 1200px !important}") as demo:
    gr.Markdown("# ğŸ¨ FastDMå›¾åƒç”Ÿæˆå™¨")
    
    with gr.Row():
        with gr.Column(scale=3):
            # ä¸»æç¤ºè¯è¾“å…¥
            prompt_input = gr.Textbox(
                label="åˆ›ä½œæç¤º",
                placeholder="è¯·è¾“å…¥æè¿°...",
                lines=3,
                max_lines=5
            )
            
            # è´Ÿå‘æç¤ºè¯
            neg_prompt = gr.Textbox(
                label="æ’é™¤å†…å®¹",
                placeholder="ä¸å¸Œæœ›å‡ºç°çš„å†…å®¹...",
                lines=1
            )
            
            # å¤šå›¾ç”Ÿå›¾åŒºåŸŸ
            with gr.Accordion("ğŸ–¼ï¸ å›¾ç‰‡ç¼–è¾‘è®¾ç½®(æ”¯æŒå¤šå›¾)", open=False) as img2img_accordion:
                with gr.Row():
                    with gr.Column():
                        # ä½¿ç”¨Galleryç»„ä»¶æ”¯æŒå¤šå›¾ç‰‡ä¸Šä¼ 
                        init_images = gr.Gallery(
                            label="upload",
                            show_label=True,
                            elem_id="gallery",
                            columns=3,
                            rows=2,
                            object_fit="contain",
                            height="auto",
                            interactive=True,
                            file_types=['image'],
                            type="pil"
                        )
                
                with gr.Row():
                    # å¤šå›¾æ··åˆæ¨¡å¼é€‰æ‹©ï¼ˆå»æ‰overlayï¼Œæ–°å¢concatenateï¼‰
                    blend_mode = gr.Dropdown(
                        choices=["concatenate", "average", "first"],
                        value="concatenate",
                        label="è¾“å…¥å›¾ç‰‡èåˆæ¨¡å¼",
                        info="concatenate: æ‹¼æ¥ | average: å¹³å‡æ··åˆ | first: ä»…ä½¿ç”¨ç¬¬ä¸€å¼ "
                    )
                    
                    # æ‹¼æ¥æ–¹å‘é€‰æ‹©ï¼ˆä»…åœ¨concatenateæ¨¡å¼ä¸‹æ˜¾ç¤ºï¼‰
                    concat_direction = gr.Dropdown(
                        choices=["horizontal", "vertical"],
                        value="horizontal",
                        label="æ‹¼æ¥æ–¹å‘",
                        info="horizontal: æ°´å¹³æ‹¼æ¥ | vertical: å‚ç›´æ‹¼æ¥",
                        visible=False  # é»˜è®¤éšè—
                    )
                
                denoise_strength = gr.Slider(
                    0.01, 1.0, 
                    value=0.75, 
                    label="å»å™ªå¼ºåº¦",
                    info="å€¼è¶Šé«˜ä¿ç•™åŸå§‹å›¾åƒè¶Šå°‘"
                )

            # å‚æ•°æ§åˆ¶åŒºåŸŸ
            with gr.Accordion("é«˜çº§å‚æ•°", open=False):
                with gr.Row():
                    steps = gr.Slider(1, 100, value=50, step=1, label="è¿­ä»£æ­¥æ•°")
                    cfg_scale = gr.Slider(1.0, 20.0, value=4.0, step=0.5, label="æç¤ºæƒé‡")
                    seed = gr.Number(label="éšæœºç§å­", value=-1)
                
                with gr.Row():
                    sampler = gr.Dropdown(
                        ["Euler a", "DPM++", "DDIM", "LMS"], 
                        value="Euler a", 
                        label="é‡‡æ ·æ–¹æ³•"
                    )
                    width = gr.Slider(0, 2048, value=512, step=64, label="å®½åº¦")
                    height = gr.Slider(0, 2048, value=512, step=64, label="é«˜åº¦")
            
            generate_btn = gr.Button("ç”Ÿæˆå›¾åƒ", variant="primary", scale=0)
        
        # è¾“å‡ºåŒºåŸŸ
        with gr.Column(scale=2):
            output_image = gr.Image(
                label="ç”Ÿæˆç»“æœ",
                type="pil",
                show_label=True
                # interactive=False,
                # height=640, width=640
            )
            gen_info = gr.Textbox(label="å‚æ•°è¯¦æƒ…", interactive=False)
            
            # æ˜¾ç¤ºå¤„ç†åçš„æ··åˆå›¾ç‰‡é¢„è§ˆ
            blended_preview = gr.Image(
                label="è¾“å…¥å›¾ç‰‡èåˆåé¢„è§ˆ",
                type="pil",
                interactive=False,
                height=300, width=300,
                visible=False
            )
    
    # ç¤ºä¾‹æç¤ºè¯
    gr.Examples(
        examples=[
            ["A futuristic cityscape with flying cars, neon lights, rain-soaked streets"],
            ["An astronaut riding a horse on Mars, photorealistic"],
            ["Watercolor painting of cherry blossoms falling on a japanese temple"]
        ],
        inputs=prompt_input
    )
    
    # æ§åˆ¶æ‹¼æ¥æ–¹å‘é€‰æ‹©å™¨çš„æ˜¾ç¤º/éšè—
    def toggle_concat_direction(blend_mode):
        if blend_mode == "concatenate":
            return gr.update(visible=True)
        else:
            return gr.update(visible=False)
    
    blend_mode.change(
        fn=toggle_concat_direction,
        inputs=blend_mode,
        outputs=concat_direction
    )
    
    # ç»‘å®šç”Ÿæˆäº‹ä»¶ï¼ˆä¼ å…¥æ‰€æœ‰å‚æ•°ï¼‰
    inputs = [
        prompt_input,
        init_images,
        blend_mode,
        concat_direction,  # æ–°å¢æ‹¼æ¥æ–¹å‘å‚æ•°
        steps,
        cfg_scale,
        seed,
        sampler,
        width,
        height
    ]
    
    generate_btn.click(
        fn=generate_image_from_prompt,
        inputs=inputs,
        outputs=output_image
    )
    
    # å›¾ç‰‡å¤„ç†é¢„è§ˆå‡½æ•°
    def preview_processed_image(images, blend_mode, concat_direction):
        if not images or len(images) == 0:
            return None, gr.update(visible=False)
        
        images = [img[0] for img in images]

        processed = process_multiple_images(images, blend_mode, concat_direction)
        if processed and (len(images) > 1 or blend_mode != "first"):
            return processed, gr.update(visible=True)
        else:
            return None, gr.update(visible=False)
    
    # å½“å›¾ç‰‡ã€å¤„ç†æ¨¡å¼æˆ–æ‹¼æ¥æ–¹å‘æ”¹å˜æ—¶ï¼Œæ˜¾ç¤ºé¢„è§ˆ
    init_images.change(
        fn=preview_processed_image,
        inputs=[init_images, blend_mode, concat_direction],
        outputs=[blended_preview, blended_preview]
    )
    
    blend_mode.change(
        fn=preview_processed_image,
        inputs=[init_images, blend_mode, concat_direction],
        outputs=[blended_preview, blended_preview]
    )
    
    concat_direction.change(
        fn=preview_processed_image,
        inputs=[init_images, blend_mode, concat_direction],
        outputs=[blended_preview, blended_preview]
    )
    
    # å‚æ•°ä¿¡æ¯æ˜¾ç¤ºå‡½æ•°
    def update_gen_info(prompt, init_images, blend_mode, concat_direction, steps, cfg_scale, seed, sampler, width, height):
        img_count = len(init_images) if init_images else 0
        mode_info = blend_mode
        if blend_mode == "concatenate" and img_count > 1:
            mode_info += f" ({concat_direction})"
        
        info = f"""
        Steps: {steps} | CFG: {cfg_scale} | Sampler: {sampler}
        Size: {width}x{height} | Seed: {seed if seed != -1 else 'random'}
        Input Images: {img_count} | Process Mode: {mode_info if img_count > 0 else 'N/A'}
        """
        return info.strip()
    
    # æ·»åŠ å‚æ•°å˜æ›´ç›‘å¬
    for component in inputs[2:]:  # ä»blend_modeå¼€å§‹çš„æ‰€æœ‰å‚æ•°
        component.change(
            fn=update_gen_info,
            inputs=inputs,
            outputs=gen_info
        )

# å¯åŠ¨æœåŠ¡ (å¯é…ç½®å‚æ•°)
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=args.port,
        share=False,  # è®¾ç½®ä¸ºTrueå¯ç”Ÿæˆä¸´æ—¶å…¬ç½‘é“¾æ¥
        show_error=True
    )