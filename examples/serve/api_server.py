#!/usr/bin/env python3
"""
åŸºäºFastDMçš„æ¨¡å‹æ¨ç†FastAPIæœåŠ¡å™¨
æ”¯æŒå¤šç§æ¶æ„ï¼šflux, qwen, wanç­‰
"""

import os
import time
import json
import base64
import io
import logging
from typing import Optional, Dict, Any, List, Union
from PIL import Image
import torch
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import HTMLResponse, Response
from pydantic import BaseModel, Field
from diffusers.utils import export_to_video
import tempfile
import uvicorn
import numpy as np
import imageio.v2 as imageio  

from fastdm.common_args import get_text_gen_parser
from fastdm.model_entry import FastDMEngine

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="FastDM æ¨ç†æœåŠ¡",
    description="åŸºäºFastDMçš„é«˜æ€§èƒ½å›¾åƒ/è§†é¢‘ç”ŸæˆAPIæœåŠ¡",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹ä¿¡æ¯
model_info: Dict[str, Any] = {}

# Pydantic æ¨¡å‹å®šä¹‰
class GenerateRequest(BaseModel):
    model: str = Field(..., description="æ¨¡å‹åç§°")
    prompt: str = Field(..., description="ç”Ÿæˆæç¤ºè¯")
    negative_prompt: Optional[str] = Field(None, description="è´Ÿå‘æç¤ºè¯")
    steps: Optional[int] = Field(default=25, description="é‡‡æ ·æ­¥æ•°")
    guidance_scale: Optional[float] = Field(default=3.5, description="å¼•å¯¼ç¼©æ”¾")
    true_cfg_scale: Optional[float] = Field(4.0, description="Qwenæ¨¡å‹ä¸“ç”¨CFGç¼©æ”¾")
    seed: int = Field(default=0, description="éšæœºç§å­ï¼Œ-1ä¸ºéšæœº")
    width: Optional[int] = Field(default=1024, description="å›¾åƒå®½åº¦")
    height: Optional[int] = Field(default=1024, description="å›¾åƒé«˜åº¦")
    num_frames: Optional[int] = Field(default=121, description="è§†é¢‘å¸§æ•°ï¼ˆwanæ¨¡å‹ï¼‰")
    fps: int = Field(default=24, description="è§†é¢‘å¸§ç‡ï¼ˆwanæ¨¡å‹ï¼‰")
    max_seq_len: Optional[int] = Field(default=512, description="æœ€å¤§åºåˆ—é•¿åº¦")
    input_image: Optional[str] = Field(None, description="base64ç¼–ç çš„æºå›¾åƒï¼Œä»…åœ¨i2vä»»åŠ¡ä¸­ä½¿ç”¨")

class EditRequest(GenerateRequest):
    input_images: Optional[Union[str, List[str]]] = Field(None, description="base64ç¼–ç çš„æºå›¾åƒ")
    blend_mode: Optional[str] = Field(
        default="list",
        description="å¤šå›¾å¤„ç†æ¨¡å¼: 'average' - å¹³å‡æ··åˆ, 'concatenate' - æ‹¼æ¥, 'first' - ä½¿ç”¨ç¬¬ä¸€å¼ , 'list' - ç›´æ¥ä¼ é€’å›¾ç‰‡åˆ—è¡¨"
    )
    concat_direction: Optional[str] = Field(
        default="horizontal",
        description="æ‹¼æ¥æ–¹å‘: 'horizontal' - æ°´å¹³æ‹¼æ¥, 'vertical' - å‚ç›´æ‹¼æ¥"
    )


class GenerateResponse(BaseModel):
    success: bool
    type: str  # "image" or "video"
    image: Optional[str] = None  # base64ç¼–ç çš„å›¾åƒ
    video: Optional[str] = None  # base64ç¼–ç çš„è§†é¢‘
    format: str
    fps: Optional[int] = None
    frames: Optional[int] = None
    generation_time: float
    model_used: str
    parameters: Optional[Dict[str, Any]] = None

class ModelInfo(BaseModel):
    model_name: str

def image_to_base64(image: Image.Image) -> str:
    """å°†PILå›¾åƒè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
    buffer = io.BytesIO()
    image.save(buffer, format='PNG')
    img_str = base64.b64encode(buffer.getvalue()).decode()
    return img_str

def base64_to_image(base64_str: str) -> Image.Image:
    """å°†base64å­—ç¬¦ä¸²è½¬æ¢ä¸ºPILå›¾åƒ"""
    img_data = base64.b64decode(base64_str)
    image = Image.open(io.BytesIO(img_data))
    return image

def batch_base64_to_images(base64_list: List[str]) -> List[Image.Image]:
    """å°†base64å­—ç¬¦ä¸²åˆ—è¡¨è½¬æ¢ä¸ºPILå›¾åƒåˆ—è¡¨"""
    images = []
    for base64_str in base64_list:
        try:
            img = base64_to_image(base64_str)
            images.append(img)
        except Exception as e:
            logger.error(f"Error decoding image: {str(e)}")
            raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„å›¾åƒæ•°æ®: {str(e)}")
    return images

#å¤šå›¾ç‰‡å¤„ç†å‡½æ•°
def process_multiple_images(images, blend_mode="list", concat_direction="horizontal"):
    """
    å¤„ç†å¤šå¼ è¾“å…¥å›¾ç‰‡
    blend_mode: "average" - å¹³å‡æ··åˆ, "concatenate" - æ‹¼æ¥, "first" - ä½¿ç”¨ç¬¬ä¸€å¼ , "list" - ç›´æ¥è¿”å›å›¾ç‰‡åˆ—è¡¨
    concat_direction: "horizontal" - æ°´å¹³æ‹¼æ¥, "vertical" - å‚ç›´æ‹¼æ¥
    """
    if not images or len(images) == 0:
        return None

    # å¦‚æœåªæœ‰ä¸€å¼ å›¾ç‰‡ï¼Œç›´æ¥è¿”å›
    if len(images) == 1:
        return images[0]

    # è½¬æ¢æ‰€æœ‰å›¾ç‰‡ä¸ºPIL Imageæ ¼å¼
    pil_images = []
    for img in images:
        if isinstance(img, np.ndarray):
            pil_img = Image.fromarray(img)
        elif isinstance(img, Image.Image):
            pil_img = img
        else:
            continue
        pil_images.append(pil_img.convert("RGB"))

    if not pil_images:
        return None

    if blend_mode == "first":
        return pil_images[0]

    elif blend_mode == "list":
        # ç›´æ¥è¿”å›å›¾ç‰‡åˆ—è¡¨ï¼Œä¸è¿›è¡Œä»»ä½•åˆå¹¶å¤„ç†
        return pil_images

    elif blend_mode == "average":
        # ç»Ÿä¸€æ‰€æœ‰å›¾ç‰‡å°ºå¯¸åˆ°ç¬¬ä¸€å¼ å›¾ç‰‡çš„å¤§å°
        base_size = pil_images[0].size
        resized_images = [img.resize(base_size, Image.Resampling.LANCZOS) for img in pil_images]

        # å¹³å‡æ··åˆ
        arrays = [np.array(img, dtype=np.float32) for img in resized_images]
        blended_array = np.mean(arrays, axis=0).astype(np.uint8)
        return Image.fromarray(blended_array)

    elif blend_mode == "concatenate":
        return concatenate_images(pil_images, concat_direction)

    return pil_images[0]

def concatenate_images(images, direction="horizontal"):
    """
    æ‹¼æ¥å¤šå¼ å›¾ç‰‡
    direction: "horizontal" - æ°´å¹³æ‹¼æ¥, "vertical" - å‚ç›´æ‹¼æ¥
    """
    if not images:
        return None
    
    if len(images) == 1:
        return images[0]
    
    if direction == "horizontal":
        # æ°´å¹³æ‹¼æ¥ï¼šç»Ÿä¸€é«˜åº¦ï¼Œå®½åº¦ç›¸åŠ 
        # æ‰¾åˆ°æœ€å°é«˜åº¦
        min_height = min(img.height for img in images)
        
        # è°ƒæ•´æ‰€æœ‰å›¾ç‰‡åˆ°ç›¸åŒé«˜åº¦ï¼Œä¿æŒå®½é«˜æ¯”
        resized_images = []
        total_width = 0
        
        for img in images:
            # è®¡ç®—æ–°å®½åº¦ä»¥ä¿æŒå®½é«˜æ¯”
            aspect_ratio = img.width / img.height
            new_width = int(min_height * aspect_ratio)
            resized_img = img.resize((new_width, min_height), Image.Resampling.LANCZOS)
            resized_images.append(resized_img)
            total_width += new_width
        
        # åˆ›å»ºæ–°ç”»å¸ƒ
        concatenated = Image.new('RGB', (total_width, min_height))
        
        # ç²˜è´´å›¾ç‰‡
        x_offset = 0
        for img in resized_images:
            concatenated.paste(img, (x_offset, 0))
            x_offset += img.width
            
        return concatenated
    
    elif direction == "vertical":
        # å‚ç›´æ‹¼æ¥ï¼šç»Ÿä¸€å®½åº¦ï¼Œé«˜åº¦ç›¸åŠ 
        # æ‰¾åˆ°æœ€å°å®½åº¦
        min_width = min(img.width for img in images)
        
        # è°ƒæ•´æ‰€æœ‰å›¾ç‰‡åˆ°ç›¸åŒå®½åº¦ï¼Œä¿æŒå®½é«˜æ¯”
        resized_images = []
        total_height = 0
        
        for img in images:
            # è®¡ç®—æ–°é«˜åº¦ä»¥ä¿æŒå®½é«˜æ¯”
            aspect_ratio = img.height / img.width
            new_height = int(min_width * aspect_ratio)
            resized_img = img.resize((min_width, new_height), Image.Resampling.LANCZOS)
            resized_images.append(resized_img)
            total_height += new_height
        
        # åˆ›å»ºæ–°ç”»å¸ƒ
        concatenated = Image.new('RGB', (min_width, total_height))
        
        # ç²˜è´´å›¾ç‰‡
        y_offset = 0
        for img in resized_images:
            concatenated.paste(img, (0, y_offset))
            y_offset += img.height
            
        return concatenated
    
    return images[0]

@app.get("/health")
async def health() -> Response:
    """Check the health of the http server."""
    return Response(status_code=200)

@app.get("/get_model_info", response_model=ModelInfo)
async def get_model_info() -> ModelInfo:
    """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
    return model_info

@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    """å›¾åƒ/è§†é¢‘ç”Ÿæˆæ¥å£"""
    log_data = request.dict()
    log_data.pop('input_images', None)  # ä¸è®°å½•input_imagesçš„base64æ•°æ®
    logger.info(f"æ¥æ”¶åˆ°ç¼–è¾‘è¯·æ±‚: {json.dumps(log_data, indent=2)}")
    # éªŒè¯å‚æ•°
    if not request.prompt.strip():
        raise HTTPException(status_code=400, detail="æç¤ºè¯ä¸èƒ½ä¸ºç©º")
    
    if request.model!=model_info.model_name:
        raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ¨¡å‹: {request.model}")
    
    if engine.task == 'i2v' and not request.input_image:
        raise HTTPException(status_code=400, detail="i2vä»»åŠ¡éœ€è¦æä¾›æºå›¾åƒ")

    # å‡†å¤‡ç”Ÿæˆå‚æ•°
    if wan_lightning:
        request.guidance_scale = 1.0
        request.steps = 4
    generate_params = {
        'prompt': request.prompt,
        'steps': request.steps,
        'guidance_scale': request.guidance_scale,
        'gen_seed': request.seed,
        'gen_width': request.width,
        'gen_height': request.height,
        'max_seq_len': request.max_seq_len
    }
    if request.negative_prompt:
        generate_params['negative_prompt'] = request.negative_prompt

    # ä¸ºä¸åŒæ¶æ„è®¾ç½®ç‰¹å®šå‚æ•°
    if engine.task in ['t2v', 'i2v']:
        # è§†é¢‘ç”Ÿæˆ
        generate_params['num_frames'] = request.num_frames 
    if engine.task == 'i2v':
        # å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆ
        try:
            input_image = base64_to_image(request.input_image)
            generate_params['src_image'] = input_image.convert("RGB")  # ç¡®ä¿æ˜¯RGBæ ¼å¼
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„æºå›¾åƒæ•°æ®: {str(e)}")

    if args.architecture == 'qwen':
        # Qwenæ¨¡å‹ä½¿ç”¨true_cfg_scale
        generate_params['true_cfg_scale'] = request.true_cfg_scale        
    
    try:
        # æ‰§è¡Œç”Ÿæˆ
        # logger.info(f"engine generate params: {json.dumps(generate_params, indent=2)}")
        gen_start_time = time.time()
        
        output = engine.generate(**generate_params)
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        
        generation_time = time.time() - gen_start_time
        
        logger.info(f"ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {generation_time:.2f}ç§’")
        
        # å¤„ç†è¾“å‡º
        if "wan" in args.architecture:
            # # è§†é¢‘è¾“å‡ºï¼šä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_file:
                temp_path = tmp_file.name
            
            export_to_video(output, temp_path, fps=request.fps)
            
            # è¯»å–è§†é¢‘æ–‡ä»¶å¹¶ç¼–ç ä¸ºbase64
            with open(temp_path, 'rb') as f:
                video_data = base64.b64encode(f.read()).decode()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_path)

            return GenerateResponse(
                success=True,
                type="video",
                video=video_data,
                format="mp4",
                fps=request.fps,
                frames=request.num_frames,
                generation_time=generation_time,
                model_used=request.model,
            )
            
        else:
            # å›¾åƒè¾“å‡º
            image_b64 = image_to_base64(output)
            
            return GenerateResponse(
                success=True,
                type="image",
                image=image_b64,
                format="png",
                generation_time=generation_time,
                model_used=request.model,
                parameters=generate_params
            )
            
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå¤±è´¥: {str(e)}")

@app.post("/edit", response_model=GenerateResponse)
async def edit_image(request: EditRequest):
    """å›¾åƒç¼–è¾‘æ¥å£"""
    log_data = request.dict()
    log_data.pop('input_images', None)  # ä¸è®°å½•input_imagesçš„base64æ•°æ®
    logger.info(f"æ¥æ”¶åˆ°ç¼–è¾‘è¯·æ±‚: {json.dumps(log_data, indent=2)}")
    # éªŒè¯å‚æ•°
    if not request.prompt.strip():
        raise HTTPException(status_code=400, detail="æç¤ºè¯ä¸èƒ½ä¸ºç©º")
    
    if request.model != model_info.model_name:
        raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ¨¡å‹: {request.model}")
    
    # å¤„ç†æºå›¾åƒ
    input_images = []
    if isinstance(request.input_images, str):
        input_images = [request.input_images]
    else:
        input_images = request.input_images
    
    assert input_images, "è¾“å…¥å›¾åƒä¸èƒ½ä¸ºç©º"
    
    try:
        input_images = batch_base64_to_images(input_images)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„å›¾åƒæ•°æ®: {str(e)}")
    processed_images = process_multiple_images(input_images, request.blend_mode, request.concat_direction)

    # å‡†å¤‡ç”Ÿæˆå‚æ•°
    generate_params = {
        'prompt': request.prompt,
        'steps': request.steps,
        'guidance_scale': request.guidance_scale,
        'gen_seed': request.seed,
        'gen_width': request.width,
        'gen_height': request.height,
        'max_seq_len': request.max_seq_len
    }

    # æ ¹æ®blend_modeå¤„ç†å›¾ç‰‡å‚æ•°
    if request.blend_mode == "list" and isinstance(processed_images, list):
        # listæ¨¡å¼ï¼šä¼ é€’å›¾ç‰‡åˆ—è¡¨
        generate_params['src_image'] = [img.convert("RGB") for img in processed_images]
    else:
        # å…¶ä»–æ¨¡å¼ï¼šä¼ é€’å•å¼ å›¾ç‰‡
        if processed_images:
            generate_params['src_image'] = processed_images.convert("RGB")
    
    if request.negative_prompt:
        generate_params['negative_prompt'] = request.negative_prompt
    
    try:
        # æ‰§è¡Œç¼–è¾‘
        gen_start_time = time.time()
        
        output = engine.generate(
            **generate_params
        )
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        
        generation_time = time.time() - gen_start_time
        
        logger.info(f"ç¼–è¾‘å®Œæˆï¼Œè€—æ—¶: {generation_time:.2f}ç§’")
        
        # å¤„ç†è¾“å‡º
        image_b64 = image_to_base64(output)
        
        return GenerateResponse(
            success=True,
            type="image",
            image=image_b64,
            format="png",
            generation_time=generation_time,
            model_used=request.model,
        )
        
    except Exception as e:
        logger.error(f"ç¼–è¾‘å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç¼–è¾‘å¤±è´¥: {str(e)}")
if __name__ == '__main__':
    parser = get_text_gen_parser()
    # æ·»åŠ FastAPIæœåŠ¡å™¨å‚æ•°
    parser.add_argument('--served-model-name', type=str, required=True, help='æ¨¡å‹åç§°')
    parser.add_argument('--host', default='0.0.0.0', help='æœåŠ¡å™¨åœ°å€')
    parser.add_argument('--port', type=int, default=8080, help='æœåŠ¡å™¨ç«¯å£')
    
    args = parser.parse_args()
        
    logger.info("ğŸš€ å¯åŠ¨ FastDM FastAPI æ¨ç†æœåŠ¡å™¨...")

    # wan lightning
    wan_lightning = False
    if args.architecture == "wan-lightning":
        args.architecture = "wan"
        wan_lightning = True
    elif args.architecture == "wan-i2v-lightning":
        args.architecture = "wan-i2v"
        wan_lightning = True

    model_load_start = time.time()
    try:
        engine = FastDMEngine(
            model_path=args.model_path,
            architecture=args.architecture,
            device=args.device,
            data_type=args.data_type,
            use_fp8=args.use_fp8,
            use_int8=args.use_int8,
            kernel_backend=args.kernel_backend,
            cache_config=args.cache_config,
            oom_resolve=args.oom_resolve,
            use_diffusers=args.use_diffusers,
            task=args.task,
        )
        model_info = ModelInfo(
            model_name=args.served_model_name
        )
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    model_load_time = time.time() - model_load_start
    logger.info(f"Model loading latency: {model_load_time:.4f} seconds")
    logger.info(f"ä»¥åŠ è½½æ¨¡å‹ä¿¡æ¯: {model_info}")

    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level="info"
    )