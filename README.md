[English ReadME](README_en.md) | [ä¸­æ–‡ ReadME](README.md)

### ç®€ä»‹

Fast Diffusion Models(FASTDM): æ˜¯ä¸€ä¸ªæ‰©æ•£æ¨¡å‹æ¨ç†å·¥ç¨‹ï¼Œå®ƒæ”¯æŒä¸»æµæ–‡ç”Ÿå›¾/è§†é¢‘æ¨¡å‹ç»“æ„ï¼Œæ”¯æŒå¤šç§GPUæ¶æ„ç®—åŠ›å¡ï¼Œæ”¯æŒcomfyuié›†æˆã€‚

å·¥ç¨‹ç»“æ„å¦‚ä¸‹ï¼š

![image](./assets/architecture.PNG)

FastDMé‡‡ç”¨æ¨¡å‹é‡åŒ–ä¸CachingæŠ€æœ¯å–å¾—äº†è¾ƒå¥½çš„æ¨ç†åŠ é€Ÿæ•ˆæœï¼Œä¸‹å›¾ä¸ºH20å¡å„æ¨¡å‹çš„latencyï¼ˆè¯¦ç»†æ€§èƒ½æ•°æ®è¯·å‚è€ƒ[performance-data](#æ€§èƒ½æ•°æ®æ±‡æ€»)ï¼‰ï¼š

![alt text](./assets/perf_graph.PNG)

FastDMæ›´å¤šå†…å®¹è¯·å‚è€ƒ[introduction](./doc/introduction.md)

### æ¨¡å‹æ”¯æŒ
ä¸šç•Œä¸»è¦æœ‰ä¸¤ç§æ¶æ„ï¼š UNET æˆ–è€… DiT, FastDMå¯¹è¿™ä¸¤ç§éƒ½è¿›è¡Œäº†é€‚é…ã€‚
#### UNET-architecthre
[StableDiffusion-XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)

[SDXL-ControlNet](https://huggingface.co/collections/diffusers/sdxl-controlnets-64f9c35846f3f06f5abe351f)
#### DiT-architecthre
[FLUX](https://huggingface.co/black-forest-labs/FLUX.1-dev)/[FLUX-Krea](https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev)/[FLUX-Kontext](https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev)

[QwenImage](https://huggingface.co/Qwen/Qwen-Image)/[Qwen-Image-Edit](https://huggingface.co/Qwen/Qwen-Image-Edit)

[StableDiffusion-3.5](https://huggingface.co/stabilityai/stable-diffusion-3.5-medium)

[Wan2.2-T2V](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B-Diffusers)

[FLUX-Controlnet](https://huggingface.co/XLabs-AI/flux-controlnet-collections)

### å®‰è£…

å¯ä»¥æºç å®‰è£…æ–¹ä¾¿ä¿®æ”¹è°ƒè¯•ï¼Œä¹Ÿå¯ä»¥é€šè¿‡dockerå®‰è£…

#### 1.æºç å®‰è£…æ–¹å¼

##### ä¾èµ–ç¯å¢ƒ

OS: Linux

Python: 3.9-3.12

GPU: compute capability 7.5 or higher (e.g., 4090, A100, H100, H20, RTX8000, L4 etc.).

CUDA-12.4 or later

torch 2.7 or later

##### å®‰è£…å‘½ä»¤

- from source

    `pip install -v -e .`

    Optional: speed up build with `pip install ninja` and set MAX_JOBS=N

#### 2.Dockerå®‰è£…æ–¹å¼
- build docker

    `docker build -d Dockerfile -t fastdm:latest .`

### ä½¿ç”¨

ä»¥ä¸‹ä¾‹ç¨‹å‡åŸºäºDiffusersçš„pipelineæ¥è¿è¡Œã€‚å½“ç„¶FastDMä¹Ÿæ”¯æŒcomfyUIï¼Œå…·ä½“å¯å‚è€ƒ[Fastdm Comfyuié›†æˆæ–‡æ¡£](./comfyui/README.md)

#### Gradio-Server

FastdDMæä¾›èµ·gradioæœåŠ¡æ­å»ºä¸€ä¸ªæ–¹ä¾¿å¿«æ·çš„ç½‘é¡µUIæ¥æ§åˆ¶å›¾ç‰‡ç”Ÿæˆã€‚
    
å¯ä»¥ä½¿ç”¨examples/serveæ–‡ä»¶å¤¹ä¸‹çš„gradio_launch.pyè„šæœ¬ï¼ŒUIå¯çµæ´»ä¿®æ”¹promptsä¸ç”Ÿæˆå‚æ•°ï¼Œä¸”æ”¯æŒå›¾ç”Ÿå›¾ï¼Œå¤šå›¾ç¼–è¾‘ç­‰ç©æ³•ã€‚

è¯¦æƒ…è¯·å‚è€ƒ[gradioæœåŠ¡demo](./examples/serve/readme.md)

#### è§†é¢‘ç”Ÿæˆ

FastDMæ”¯æŒWan2.2æ¨¡å‹è¿›è¡Œè§†é¢‘ç”Ÿæˆã€‚ç”±äºA14Bç‰ˆæœ¬æ¨ç†è€—æ—¶éå¸¸é•¿ï¼Œæˆ‘ä»¬å¼ºçƒˆæ¨èä½¿ç”¨[Wan2.2-Lightning](https://github.com/ModelTC/Wan2.2-Lightning)çš„è’¸é¦æ¨¡å‹ã€‚å®ƒå¤§å¹…å‡å°‘æ¨ç†stepsï¼Œå¤§å¹…æå‡äº†ç”Ÿæˆé€Ÿåº¦ã€‚

å¯ä»¥ä»[è¯¥åœ°å€](https://huggingface.co/FastDM/Wan2.2-T2V-A14B-Merge-Lightning-V1.0-Diffusers)ä¸‹è½½æˆ‘ä»¬Mergeå¥½çš„wan2.2-lightingï¼Œä½¿ç”¨FastDMè¿›è¡Œæ¨ç†ã€‚

`python gen.py --model-path /path/to/Wan2.2-T2V-A14B-Merge-Lightning-V1.1-Diffusers --architecture wan --guidance-scale 1.0 --height 512 --width 512 --steps 4 --use-fp8 --output-path ./wan-a14b-lightningv1.1-fp8-guid1.mp4 --num-frames 81 --fps 16 --prompts "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage." --task i2v`

ä»¥ä¸Šå‘½ä»¤ç”Ÿæˆä¸€ä¸ª5sï¼ˆ81/16=5ï¼‰çš„è§†é¢‘ï¼Œåœ¨H20ä¸Šåªéœ€23sï¼Œéå¸¸è¿…é€Ÿã€‚

#### Text2Image:

ä¸ä½¿ç”¨serveræ¨¡å¼ï¼Œä¹Ÿå¯ä»¥ç›´æ¥è¿è¡Œexamples/demoæ–‡ä»¶å¤¹ä¸‹çš„gen.pyè„šæœ¬è¿›è¡Œç”Ÿå›¾ï¼š

`python gen.py --model-path /path/to/FLUX.1-Krea-dev --architecture flux --height 1024 --width 2048 --steps 25 --use-fp8 --output-path ./flux-fp8.png --prompts "A frog holding a sign that says hello world"`

##### å°æ˜¾å­˜å¡è·‘Qwen-Image/FLUX(æ¯”å¦‚4090D-24GB)

Qwen-Imageæ¨¡å‹é€šå¸¸éœ€è¦äº”å…­åGBæ˜¾å­˜æ‰å¯ä»¥è¿è¡Œ, FLUXæ¨¡å‹éœ€è¦>24GB, å¦åˆ™ä¼šOOMã€‚å¯ä»¥é…ç½®`--oom-resolve`å‡å°‘æ˜¾å­˜å ç”¨ï¼Œè¿™æ ·4090/4090D-24Gï¼ŒA100-40Gç­‰å°æ˜¾å­˜çš„å¡å°±éƒ½å¯ä»¥æ”¯æŒã€‚

**æ³¨æ„**ï¼šè¯¥æ¨¡å¼å°†text-encodeéƒ¨åˆ†åœ¨cpuè¿è¡Œï¼Œä¼šæ‹–æ…¢ç”Ÿæˆé€Ÿåº¦ã€‚qwen-imageæ¨¡å‹åœ¨24GBæ˜¾å­˜å¡ä¸Šä¼šé‡åŒ–æ›´å¤šéƒ¨åˆ†ï¼Œå½±å“ä¸€äº›ç”Ÿæˆæ•ˆæœã€‚

`python gen.py --model-path /path/to/qwen-image --use-int8 --architecture qwen --output-path ./qwen-int8-tmp.png --oom-resolve --cache-config ../xcaching/configs/teacache_qwenimage.json --width 768 --height 768`

**æ³¨**: ä½¿ç”¨24Gæ˜¾å­˜å¡ï¼Œè¿™ç§æ¨¡å¼ä¸‹ç”Ÿæˆå›¾ç‰‡å°ºå¯¸å»ºè®®å°äº768x768ï¼Œå¦åˆ™vaeéƒ¨åˆ†ä¹Ÿä¼šå ç”¨å¾ˆå¤šæ˜¾å­˜ï¼Œé€ æˆOOMã€‚

**æ³¨**: å½“å‰FastDMè·‘Qwen-Imageæœ€å°ä¹Ÿéœ€è¦20GBä»¥ä¸Šçš„æ˜¾å­˜ï¼Œå½“å‰è¿˜ä¸æ”¯æŒ<20GBçš„å¡è·‘ï¼Œåé¢æ”¯æŒ4bitæƒé‡é‡åŒ–ä¹‹ååº”è¯¥å¯ä»¥åœ¨æ›´ä½æ˜¾å­˜å¡ä¸Šè¿è¡Œã€‚

#### LORA:

ç¤¾åŒºä¸­æœ‰å¾ˆå¤šloraæ¨¡å‹å¸¦æ¥äº†ç”Ÿæˆå›¾ç‰‡çš„çœŸå®æ€§ï¼Œå¤šæ ·æ€§å’Œä¸°å¯Œæ€§ã€‚FastDMæ”¯æŒloraæ¨¡å‹çš„åŠ è½½å’Œä½¿ç”¨ã€‚

è¯¦æƒ…è¯·å‚è€ƒ[loraç”Ÿæˆdemo](./examples/lora-gen/readme.md)

#### Image-Editing:

ä½¿ç”¨examples/demoæ–‡ä»¶å¤¹ä¸‹çš„image_edit.pyè„šæœ¬å¯¹å›¾ç‰‡è¿›è¡Œç¼–è¾‘(æ”¯æŒqwen-img-editå’ŒFLUX.1-Kontext-devæ¨¡å‹):

`python image_edit.py --model-path /path/to/Image-Edit-Model --use-int8 --image-path ./ast_ride_horse.png --prompts "Change the horse's color to purple, with a flash light background."`

ä¸Text2Imageç±»ä¼¼, å»ºè®®ä½¿ç”¨gradio_launch.pyè„šæœ¬æ­å»ºä¸€ä¸ªwebæœåŠ¡æ¥è¿›è¡Œå›¾ç‰‡ç¼–è¾‘ï¼Œç•…ç©ç±»ä¼¼**nano-banana**çš„å¤šå›¾ç¼–è¾‘æ¨¡å¼ã€‚è¯¦æƒ…è¯·å‚è€ƒ[gradioæœåŠ¡demo](./examples/serve/readme.md)

![image](./assets/img-edit.PNG)

#### Controlnet:

ä½¿ç”¨examples/demoæ–‡ä»¶å¤¹ä¸‹çš„contrlnet_demo.pyè„šæœ¬, éœ€è¦ä¿®æ”¹diffuserså¯¹åº”pipelineçš„ä»£ç , å¯å‚è€ƒexamples/demoä¸‹çš„readmeæ–‡æ¡£ã€‚

### æ€§èƒ½æ•°æ®æ±‡æ€»

text2imageï¼š

  all-models: **height = 1024ï¼Œwidth = 2048ï¼Œnum_inference_steps = 25**

text2videoï¼š
    
  wan-5B: **height = 768ï¼Œwidth = 768ï¼Œnum_frames = 121ï¼Œnum_inference_steps = 50**
    
  wan-A14Bï¼š**height = 720ï¼Œwidth = 1280ï¼Œnum_frames = 81ï¼Œnum_inference_steps = 40**

**æ³¨**ï¼šä»¥ä¸‹æ•°æ®ä¸­ï¼ŒH20æ€§èƒ½æ•°æ®ä½¿ç”¨äº†[SageAttention](https://github.com/thu-ml/SageAttention)ã€‚SageAttentionæ€§èƒ½æ¯”torch-sdpaç®—å­æœ‰è¾ƒå¤§æå‡ï¼Œè¯¦æƒ…å¯å‚è€ƒè¯¥[å¼€æºå·¥ç¨‹](https://github.com/thu-ml/SageAttention)ã€‚å¦‚æœç¯å¢ƒä¸­å®‰è£…äº†SageAttentionï¼ŒFastDMçš„CUDA-backendæ¨¡å¼ä¸‹ä¼šç›´æ¥è°ƒç”¨ã€‚

Qwen-Imageçš„A100ä¸RTX-8000æ•°æ®ä½¿èƒ½äº†`--oom-resolve`ä»¥è§£å†³OOMé—®é¢˜

<a id="perf"></a>
![image](./assets/perf.PNG)


### æ¨¡å‹ç²¾åº¦æµ‹è¯•

ä½¿ç”¨examples/evaluationæ–‡ä»¶å¤¹ä¸‹çš„clip_score.pyå’Œfid.pyè„šæœ¬è®¡ç®—æµ‹è¯„åˆ†æ•°(æ›´å¤šå†…å®¹è¯·å‚è€ƒ[evaluation](./examples/evaluation/README.md))ï¼š

### Acknowledgement

We learned the design and reused code from the following projects: [Diffusers](https://github.com/huggingface/diffusers), [vLLM](https://github.com/vllm-project/vllm), [Flash-attention](https://github.com/Dao-AILab/flash-attention), [SGLang](https://github.com/sgl-project/sglang), [teacache](https://github.com/ali-vilab/TeaCache)

The cuda-backend kernels(high performance operator, [cutlass](https://github.com/NVIDIA/cutlass/tree/v4.1.0)-based gemm or self-attention-fp8) implementations adapted from vllm or sglang kernels and flash-attention. In order to clone the Cutlass source code from GitHub without using git submodule(the domestic network is often disconnected if you don't use VPN), we directly put the Cutlass header files in the csrc/include, this method is rather crudeğŸ˜‚.

Thanks to the distillation lora models of wan2.2 the [ModelTC community](https://github.com/ModelTC/Wan2.2-Lightning) provides. We merge the [wan2.2-lightning-model](https://github.com/ModelTC/Wan2.2-Lightning) and wan2.2 base model to an new model. It significantly increases the speed of video generation.