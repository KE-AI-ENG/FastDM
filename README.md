[English ReadME](README_en.md) | [ä¸­æ–‡ ReadME](README.md)

### ç®€ä»‹

Fast Diffusion Models(FASTDM): æ˜¯ä¸€ä¸ªæ‰©æ•£æ¨¡å‹æ¨ç†å·¥ç¨‹ï¼Œå®ƒæ”¯æŒä¸»æµæ–‡ç”Ÿå›¾/è§†é¢‘æ¨¡å‹ç»“æ„ï¼Œæ”¯æŒå¤šç§GPUæ¶æ„ç®—åŠ›å¡ï¼Œæ”¯æŒcomfyuié›†æˆã€‚

FastDMå¯¹æ˜¾å­˜å ç”¨ä¹Ÿè¿›è¡Œäº†ä¼˜åŒ–ï¼Œè¾ƒå¤§çš„Qwen-Imageç³»åˆ—æ¨¡å‹ï¼Œ24GBæ˜¾å­˜çš„å¡ä¹Ÿå¯ä»¥è·‘é€šã€‚

å·¥ç¨‹æ•´ä½“ç»“æ„å¦‚ä¸‹ï¼š

![image](./assets/architecture.PNG)

æ›´å¤šå†…å®¹è¯·å‚è€ƒ[introduction](./doc/introduction.md)  

FastDMæ¨¡å‹åŠ é€Ÿæ•ˆæœï¼ˆæ›´å¤šå†…å®¹è¯·å‚è€ƒ[æ€§èƒ½è¯¦ç»†æ•°æ®](#perf)ï¼‰ï¼š
![alt text](./assets/perf_graph.PNG)
### æ¨¡å‹æ”¯æŒ
ä¸šç•Œä¸»è¦æœ‰ä¸¤ç§æ¶æ„ï¼š UNET æˆ–è€… DiT, FastDMå¯¹è¿™ä¸¤ç§éƒ½è¿›è¡Œäº†é€‚é…ã€‚
#### UNET-architecthre
[StableDiffusion-XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)

[SDXL-ControlNet](https://huggingface.co/collections/diffusers/sdxl-controlnets-64f9c35846f3f06f5abe351f)
#### DiT-architecthre
[FLUX](https://huggingface.co/black-forest-labs/FLUX.1-dev)/[FLUX-Krea](https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev)/[FLUX-Kontext](https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev)

[QwenImage](https://huggingface.co/Qwen/Qwen-Image)/[Qwen-Image-Edit](https://huggingface.co/Qwen/Qwen-Image-Edit)

[StableDiffusion-3.5](https://huggingface.co/stabilityai/stable-diffusion-3.5-medium)

[Wan2.2-T2V](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B-Diffusers)

[FLUX-Controlnet](https://huggingface.co/XLabs-AI/flux-controlnet-collections)

### å®‰è£…

å¯ä»¥æºç å®‰è£…æ–¹ä¾¿ä¿®æ”¹è°ƒè¯•ï¼Œä¹Ÿå¯ä»¥é€šè¿‡dockerå®‰è£…

#### 1.æºç å®‰è£…æ–¹å¼

##### ä¾èµ–ç¯å¢ƒ

OS: Linux

Python: 3.9-3.12

GPU: compute capability 7.5 or higher (e.g., 4090, A100, H100, H20, RTX8000, L4 etc.).

CUDA-12.4 or later

torch 2.7 or later

##### å®‰è£…å‘½ä»¤

- from source

    `pip install -v -e .`

    Optional: speed up build with `pip install ninja` and set MAX_JOBS=N

#### 2.Dockerå®‰è£…æ–¹å¼
- build docker

    `docker build -d Dockerfile -t fastdm:latest .`

### ä½¿ç”¨

examplesåŒ…æ‹¬Text2Image, LORA, Image-Editing, Controlnetç­‰demo, ä»¥ä¸‹ä¾‹ç¨‹å‡åŸºäºdiffusersçš„pipelineæ¥è¿è¡Œã€‚

å½“ç„¶, FastDMä¹Ÿæ”¯æŒcomfyUIé›†æˆï¼Œå…·ä½“å¯å‚è€ƒ[Fastdm Comfyuié›†æˆæ–‡æ¡£](./comfyui/README.md)

#### Text2Image:

ä½¿ç”¨examples/demoæ–‡ä»¶å¤¹ä¸‹çš„gen.pyè„šæœ¬ç”Ÿæˆä¸€å¼ å›¾ç‰‡ï¼š

`python gen.py --model-path /path/to/FLUX.1-Krea-dev --architecture flux --height 1024 --width 2048 --steps 25 --use-fp8 --output-path ./flux-fp8.png --prompts "A frog holding a sign that says hello world"`

##### Gradio-Server

ä¸Šè¿°ä½¿ç”¨pythonè„šæœ¬ç”Ÿæˆå›¾ç‰‡è¾ƒç¹çï¼Œé€šå¸¸æˆ‘ä»¬æƒ³è¦ä¸€ä¸ªæ–¹ä¾¿å¿«æ·çš„ç½‘é¡µUIæ¥æ§åˆ¶å›¾ç‰‡ç”Ÿæˆã€‚
    
å¯ä»¥ä½¿ç”¨examples/serveæ–‡ä»¶å¤¹ä¸‹çš„gradio_launch.pyè„šæœ¬å¿«é€Ÿæ­å»ºä¸€ä¸ªwebæœåŠ¡ï¼Œè¿™æ ·å°±å¯ä»¥é€šè¿‡æµè§ˆå™¨ç½‘é¡µè§¦å‘å›¾ç‰‡ç”Ÿæˆï¼Œå¯çµæ´»ä¿®æ”¹promptsä¸ç”Ÿæˆå‚æ•°ã€‚è¯¦æƒ…è¯·å‚è€ƒ[gradioæœåŠ¡demo](./examples/serve/readme.md)

##### å°æ˜¾å­˜å¡è·‘Qwen-Image/FLUX(æ¯”å¦‚4090D-24GB)

Qwen-Imageæ¨¡å‹é€šå¸¸éœ€è¦äº”å…­åGBæ˜¾å­˜æ‰å¯ä»¥è¿è¡Œ, FLUXæ¨¡å‹éœ€è¦>24GB, å¦åˆ™ä¼šOOMã€‚å¯ä»¥é…ç½®`--oom-resolve`ï¼Œä½¿å…¶åªéœ€20å¤šGBæ˜¾å­˜å³å¯è¿è¡Œï¼Œè¿™æ ·A100å’Œ4090/4090Dï¼ŒRTX-8000ç­‰å°æ˜¾å­˜çš„å¡å°±éƒ½å¯ä»¥æ”¯æŒã€‚æ³¨æ„è¯¥æ¨¡å¼å°†text-encodeéƒ¨åˆ†åœ¨cpuè¿è¡Œï¼Œä¼šæ‹–æ…¢ç”Ÿæˆé€Ÿåº¦ã€‚qwen-imageæ¨¡å‹åœ¨<24GBæ˜¾å­˜å¡ä¸Šä¼šé‡åŒ–æ›´å¤šéƒ¨åˆ†ï¼Œå½±å“ä¸€äº›ç”Ÿæˆæ•ˆæœã€‚

`python gen.py --model-path /path/to/qwen-image --use-int8 --architecture qwen --output-path ./qwen-int8-tmp.png --oom-resolve --cache-config ../xcaching/configs/teacache_qwenimage.json --width 768 --height 768`

**æ³¨**: è¿™ç§æ¨¡å¼ä¸‹ç”Ÿæˆå›¾ç‰‡å°ºå¯¸å»ºè®®å°äº768x768ï¼Œå¦åˆ™vaeéƒ¨åˆ†ä¹Ÿä¼šå ç”¨å¾ˆå¤šæ˜¾å­˜ï¼Œé€ æˆOOMã€‚

**æ³¨**: å½“å‰FastDMè·‘Qwen-Imageæœ€å°ä¹Ÿéœ€è¦20GBä»¥ä¸Šçš„æ˜¾å­˜ï¼Œå½“å‰è¿˜ä¸æ”¯æŒ<20GBçš„å¡è·‘ï¼Œåé¢æ”¯æŒ4bitæƒé‡é‡åŒ–ä¹‹ååº”è¯¥å¯ä»¥åœ¨æ›´ä½æ˜¾å­˜å¡ä¸Šè·‘Qwen-Imageã€‚

#### LORA:

ç¤¾åŒºä¸­æœ‰å¾ˆå¤šloraæ¨¡å‹å¸¦æ¥äº†ç”Ÿæˆå›¾ç‰‡çš„çœŸå®æ€§ï¼Œå¤šæ ·æ€§å’Œä¸°å¯Œæ€§ã€‚FastDMæ”¯æŒloraæ¨¡å‹çš„åŠ è½½å’Œä½¿ç”¨ã€‚

è¯¦æƒ…è¯·å‚è€ƒ[loraç”Ÿæˆdemo](./examples/lora-gen/readme.md)

#### Image-Editing:

ä½¿ç”¨examples/demoæ–‡ä»¶å¤¹ä¸‹çš„image_edit.pyè„šæœ¬å¯¹å›¾ç‰‡è¿›è¡Œç¼–è¾‘(æ”¯æŒqwen-img-editå’ŒFLUX.1-Kontext-devæ¨¡å‹):

`python image_edit.py --model-path /path/to/Image-Edit-Model --use-int8 --image-path ./ast_ride_horse.png --prompts "Change the horse's color to purple, with a flash light background."`

ä¸Text2Imageç±»ä¼¼, ä¹Ÿå¯ä»¥ä½¿ç”¨gradio_launch.pyè„šæœ¬æ­å»ºä¸€ä¸ªwebæœåŠ¡æ¥è¿›è¡Œå›¾ç‰‡ç¼–è¾‘ï¼Œç•…ç©ç±»ä¼¼**nano-banana**çš„å¤šå›¾ç¼–è¾‘æ¨¡å¼ã€‚è¯¦æƒ…è¯·å‚è€ƒ[gradioæœåŠ¡demo](./examples/serve/readme.md)

![image](./assets/img-edit.PNG)

#### Controlnet:

ä½¿ç”¨examples/demoæ–‡ä»¶å¤¹ä¸‹çš„contrlnet_demo.pyè„šæœ¬, éœ€è¦ä¿®æ”¹diffuserså¯¹åº”pipelineçš„ä»£ç , å¯å‚è€ƒexamples/demoä¸‹çš„readmeæ–‡æ¡£ã€‚

### æ€§èƒ½æ•°æ®æ±‡æ€»

text2imageï¼š

  all-models: **height = 1024ï¼Œwidth = 2048ï¼Œnum_inference_steps = 25**

text2videoï¼š
    
  wan-5B: **height = 704ï¼Œwidth = 1280ï¼Œnum_frames = 121ï¼Œnum_inference_steps = 50**
    
  wan-A14Bï¼š**height = 720ï¼Œwidth = 1280ï¼Œnum_frames = 81ï¼Œnum_inference_steps = 40**

**æ³¨**ï¼šä»¥ä¸‹æ•°æ®ä¸­ï¼ŒH20æ€§èƒ½æ•°æ®ä½¿ç”¨äº†[SageAttention](https://github.com/thu-ml/SageAttention)ã€‚SageAttentionæ€§èƒ½æ¯”torch-sdpaç®—å­æœ‰è¾ƒå¤§æå‡ï¼Œè¯¦æƒ…å¯å‚è€ƒè¯¥[å¼€æºå·¥ç¨‹](https://github.com/thu-ml/SageAttention)ã€‚å¦‚æœç¯å¢ƒä¸­å®‰è£…äº†SageAttentionï¼ŒFastDMçš„CUDA-backendæ¨¡å¼ä¸‹ä¼šç›´æ¥è°ƒç”¨ã€‚

Qwen-Imageçš„A100ä¸RTX-8000æ•°æ®ä½¿èƒ½äº†`--oom-resolve`ä»¥è§£å†³OOMé—®é¢˜

<a id="perf"></a>
![image](./assets/perf.PNG)


### æ¨¡å‹ç²¾åº¦æµ‹è¯•

ä½¿ç”¨examples/evaluationæ–‡ä»¶å¤¹ä¸‹çš„clip_score.pyå’Œfid.pyè„šæœ¬è®¡ç®—æµ‹è¯„åˆ†æ•°(æ›´å¤šå†…å®¹è¯·å‚è€ƒ[evaluation](./examples/evaluation/README.md))ï¼š

### Acknowledgement

We learned the design and reused code from the following projects: [Diffusers](https://github.com/huggingface/diffusers), [vLLM](https://github.com/vllm-project/vllm), [Flash-attention](https://github.com/Dao-AILab/flash-attention), [SGLang](https://github.com/sgl-project/sglang), [teacache](https://github.com/ali-vilab/TeaCache)

The cuda-backend kernels(high performance operator, [cutlass](https://github.com/NVIDIA/cutlass/tree/v4.1.0)-based gemm or self-attention-fp8) implementations adapted from vllm or sglang kernels and flash-attention. In order to clone the Cutlass source code from GitHub without using git submodule(the domestic network is often disconnected if you don't use VPN), we directly put the Cutlass header files in the csrc/include, this method is rather crudeğŸ˜‚.