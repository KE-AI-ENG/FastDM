[English ReadME](README_en.md) | [ä¸­æ–‡ ReadME](README.md)

### ç®€ä»‹

Fast Diffusion Models(FASTDM): æ˜¯ä¸€ä¸ªæ‰©æ•£æ¨¡å‹æ¨ç†å·¥ç¨‹ï¼Œå®ƒæ”¯æŒä¸»æµæ–‡ç”Ÿå›¾/è§†é¢‘æ¨¡å‹ç»“æ„ï¼Œæ”¯æŒcomfyuié›†æˆï¼Œæ”¯æŒå¤šç§GPUæ¶æ„ç®—åŠ›å¡ã€‚

å·¥ç¨‹æ•´ä½“ç»“æ„å¦‚ä¸‹ï¼š

![image](./assets/architecture.PNG)

æ›´å¤šå†…å®¹è¯·å‚è€ƒ[introduction](./doc/introduction.md)

### æ¨¡å‹æ”¯æŒ
ä¸šç•Œä¸»è¦æœ‰ä¸¤ç§æ¶æ„ï¼š UNET æˆ–è€… DiT, FastDMå¯¹è¿™ä¸¤ç§éƒ½è¿›è¡Œäº†é€‚é…ã€‚
#### UNET-architecthre
[StableDiffusion-XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)

[SDXL-ControlNet](https://huggingface.co/collections/diffusers/sdxl-controlnets-64f9c35846f3f06f5abe351f)
#### DiT-architecthre
[FLUX](https://huggingface.co/black-forest-labs/FLUX.1-dev)/[FLUX-Krea](https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev)/[FLUX-Kontext](https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev)

[QwenImage](https://huggingface.co/Qwen/Qwen-Image)/[Qwen-Image-Edit](https://huggingface.co/Qwen/Qwen-Image-Edit)

[StableDiffusion-3.5](https://huggingface.co/stabilityai/stable-diffusion-3.5-medium)

[Wan2.2-T2V](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B-Diffusers)

[FLUX-Controlnet](https://huggingface.co/XLabs-AI/flux-controlnet-collections)

### å®‰è£…

å¯ä»¥æºç å®‰è£…æ–¹ä¾¿ä¿®æ”¹è°ƒè¯•ï¼Œä¹Ÿå¯ä»¥é€šè¿‡dockerå®‰è£…

#### 1.æºç å®‰è£…æ–¹å¼

##### ä¾èµ–ç¯å¢ƒ

OS: Linux

Python: 3.9-3.12

GPU: compute capability 7.5 or higher (e.g., 4090, A100, H100, H20, RTX8000, L4 etc.).

CUDA-12.4 or later

torch 2.7 or later

##### å®‰è£…å‘½ä»¤

- from source

    `pip install -v -e .`

    Optional: speed up build with `pip install ninja` and set MAX_JOBS=N

#### 2.Dockerå®‰è£…æ–¹å¼
- build docker

    `docker build -d Dockerfile -t fastdm:latest .`

### ä½¿ç”¨

examplesåŒ…æ‹¬Text2Image, LORA, Image-Editing, Controlnetç­‰demo, è¿™äº›ä¾‹ç¨‹å‡åŸºäºdiffusersçš„pipeline, è¯¦æƒ…å¦‚ä¸‹ã€‚

åŒæ—¶, FastDMä¹Ÿæ”¯æŒcomfyUIé›†æˆï¼Œå…·ä½“å¯å‚è€ƒ[comfyuié›†æˆ](./comfyui/README.md)

#### Text2Image:

- diffusers pipelineä¸­ä½¿ç”¨fastdm:
    ä½¿ç”¨examples/demoæ–‡ä»¶å¤¹ä¸‹çš„gen.pyè„šæœ¬ç”Ÿæˆä¸€å¼ å›¾ç‰‡ï¼š

    `python gen.py --model-path /path/to/FLUX.1-Krea-dev --architecture flux --height 1024 --width 2048 --steps 25 --use-fp8 --output-path ./flux-fp8.png --prompts "A frog holding a sign that says hello world"`

    **æ³¨**: Qwen-Imageæ¨¡å‹é€šå¸¸éœ€è¦äº”åå¤šGBæ˜¾å­˜æ‰å¯ä»¥è¿è¡Œï¼Œå¦åˆ™ä¼šOOMï¼Œå¯ä»¥é…ç½®`--qwen-oom-resolve`ï¼Œä½¿å…¶åªéœ€26GBå·¦å³å³å¯è¿è¡Œï¼Œè¿™æ ·A100-40Gå’Œ4090ï¼ŒRTX-8000ç­‰48GBæ˜¾å­˜çš„å¡å°±éƒ½å¯ä»¥è¿è¡Œã€‚å®ƒä¼šä½¿Transformerä¸vaeéƒ¨åˆ†åœ¨gpuè¿è¡Œï¼Œtext_encoderéƒ¨åˆ†åœ¨cpuè¿è¡Œã€‚

    ä½¿ç”¨pythonè„šæœ¬ç”Ÿæˆå›¾ç‰‡è¾ƒç¹çï¼Œé€šå¸¸æˆ‘ä»¬æƒ³è¦ä¸€ä¸ªæ–¹ä¾¿å¿«æ·çš„ç½‘é¡µUIæ¥æ§åˆ¶å›¾ç‰‡ç”Ÿæˆã€‚
    
    å¯ä»¥ä½¿ç”¨examples/serveæ–‡ä»¶å¤¹ä¸‹çš„gradio_launch.pyè„šæœ¬å¿«é€Ÿæ­å»ºä¸€ä¸ªwebæœåŠ¡ï¼Œè¿™æ ·å°±å¯ä»¥é€šè¿‡æµè§ˆå™¨ç½‘é¡µè§¦å‘å›¾ç‰‡ç”Ÿæˆï¼Œå¯çµæ´»ä¿®æ”¹promptsä¸ç”Ÿæˆå‚æ•°ã€‚è¯¦æƒ…è¯·å‚è€ƒ[gradioæœåŠ¡demo](./examples/serve/readme.md)

- comfyUIä¸­ä½¿ç”¨fastdm:

    è¯·å‚è€ƒ[Fastdm Comfyuiä½¿ç”¨æ–‡æ¡£](./comfyui//README.md)
    
    
#### LORA:

ç¤¾åŒºä¸­æœ‰å¾ˆå¤šloraæ¨¡å‹å¸¦æ¥äº†ç”Ÿæˆå›¾ç‰‡çš„çœŸå®æ€§ï¼Œå¤šæ ·æ€§å’Œä¸°å¯Œæ€§ã€‚FastDMæ”¯æŒloraæ¨¡å‹çš„åŠ è½½å’Œä½¿ç”¨ã€‚

è¯¦æƒ…è¯·å‚è€ƒ[loraç”Ÿæˆdemo](./examples/lora-gen/readme.md)

#### Image-Editing:

ä½¿ç”¨examples/demoæ–‡ä»¶å¤¹ä¸‹çš„image_edit.pyè„šæœ¬å¯¹å›¾ç‰‡è¿›è¡Œç¼–è¾‘(æ”¯æŒqwen-img-editå’ŒFLUX.1-Kontext-devæ¨¡å‹):

`python image_edit.py --model-path /path/to/Image-Edit-Model --use-int8 --image-path ./ast_ride_horse.png --prompts "Change the horse's color to purple, with a flash light background."`

ä¸Text2Imageç±»ä¼¼, ä¹Ÿå¯ä»¥ä½¿ç”¨gradio_launch.pyè„šæœ¬æ­å»ºä¸€ä¸ªwebæœåŠ¡æ¥è¿›è¡Œå›¾ç‰‡ç¼–è¾‘ï¼Œè¯¦æƒ…è¯·å‚è€ƒ[gradioæœåŠ¡demo](./examples/serve/readme.md)

![image](./assets/img-edit.PNG)

#### Controlnet:

ä½¿ç”¨examples/demoæ–‡ä»¶å¤¹ä¸‹çš„contrlnet_demo.pyè„šæœ¬, éœ€è¦ä¿®æ”¹diffuserså¯¹åº”pipelineçš„ä»£ç , å¯å‚è€ƒexamples/demoä¸‹çš„readmeæ–‡æ¡£ã€‚

### æ€§èƒ½æ•°æ®æ±‡æ€»

text2imageï¼š

  all-models: **height = 1024ï¼Œwidth = 2048ï¼Œnum_inference_steps = 25**

text2videoï¼š
    
  wan-5B: **height = 704ï¼Œwidth = 1280ï¼Œnum_frames = 121ï¼Œnum_inference_steps = 50**
    
  wan-A14Bï¼š**height = 720ï¼Œwidth = 1280ï¼Œnum_frames = 81ï¼Œnum_inference_steps = 40**

æ³¨ï¼šä»¥ä¸‹æ•°æ®ä¸­ï¼Œqwen-imageä¸wançš„H20æ€§èƒ½æ•°æ®ä½¿ç”¨äº†[SageAttention](https://github.com/thu-ml/SageAttention), å…¶ä»–æ¨¡å‹å’Œå¡å‹éƒ½æœªä½¿ç”¨ã€‚SageAttentionæ€§èƒ½æ¯”torch-sdpaç®—å­æœ‰è¾ƒå¤§æå‡ï¼Œè¯¦æƒ…å¯å‚è€ƒè¯¥[å¼€æºå·¥ç¨‹](https://github.com/thu-ml/SageAttention)ã€‚å¦‚æœç¯å¢ƒä¸­å®‰è£…äº†SageAttentionï¼ŒFastDMçš„CUDA-backendæ¨¡å¼ä¸‹ä¼šç›´æ¥è°ƒç”¨ã€‚

![image](./assets/perf.PNG)

### æ¨¡å‹ç²¾åº¦æµ‹è¯•

ä½¿ç”¨examples/evaluationæ–‡ä»¶å¤¹ä¸‹çš„clip_score.pyå’Œfid.pyè„šæœ¬è®¡ç®—æµ‹è¯„åˆ†æ•°(æ›´å¤šå†…å®¹è¯·å‚è€ƒ[evaluation](./examples/evaluation/README.md))ï¼š

### Acknowledgement

We learned the design and reused code from the following projects: [Diffusers](https://github.com/huggingface/diffusers), [vLLM](https://github.com/vllm-project/vllm), [Flash-attention](https://github.com/Dao-AILab/flash-attention), [SGLang](https://github.com/sgl-project/sglang), [teacache](https://github.com/ali-vilab/TeaCache)

The cuda-backend kernels(high performance operator, [cutlass](https://github.com/NVIDIA/cutlass/tree/v4.1.0)-based gemm or self-attention-fp8) implementations adapted from vllm or sglang kernels and flash-attention. In order to clone the Cutlass source code from GitHub without using git submodule(the domestic network is often disconnected if you don't use VPN), we directly put the Cutlass header files in the csrc/include, this method is rather crudeğŸ˜‚.